---
name: generate-detailed-report
description: Generate comprehensive grading report with specific issues, file paths, and actionable recommendations
version: 1.0.0
---

# Generate Detailed Grading Report Skill

Creates a comprehensive, actionable grading report that goes beyond scores to provide:
- Specific file paths and line numbers for violations
- Exact issues found in each category
- Actionable fix recommendations
- Prioritized action items by point value

## When to Use This Skill

After running all grading skills, use this to generate a detailed report that helps students understand:
- **Exactly** what files have problems
- **Exactly** what needs to be fixed
- **Exactly** how to improve their score

## Input Required

- **Project path**: Path to the graded project
- **Grading results**: Complete results from all grading skills

## Instructions

### Step 1: Gather Complete Grading Results

Run all grading skills **in parallel** to get detailed results:

```bash
# Run all 7 grading skills in parallel groups
# Group 1 (parallel)
/skill check-security <project_path>
/skill validate-docs <project_path>
/skill check-ux <project_path>

# Group 2 (parallel)
/skill analyze-code <project_path>
/skill evaluate-tests <project_path>

# Group 3 (parallel)
/skill assess-git <project_path>
/skill grade-research <project_path>
```

Or let the grade-project agent orchestrate everything automatically.

### Step 2: Analyze Each Category in Detail

For each grading category, extract and present:

#### ğŸ”’ SECURITY (10 points)

**Show specifically:**
- File paths where secrets were found (if any)
- Which .gitignore patterns are missing
- Whether .env.example exists

**Format:**
```
SECURITY: X/10 points

[If secrets found:]
âœ— CRITICAL: Hardcoded secrets detected
  Location: src/config.py:23 (API key)
  Location: utils/db.py:45 (password)
  Fix: Move to .env file, add to .gitignore

[If .gitignore issues:]
âœ— .gitignore missing required patterns
  Missing: .env, *.key, *.pem, __pycache__
  Fix: Add these patterns to .gitignore

[If .env.example missing:]
âœ— No .env.example template found
  Fix: Create .env.example with:
    API_KEY=your_api_key_here
    DB_PASSWORD=your_password_here
```

#### ğŸ’» CODE QUALITY (30 points)

**Show specifically:**
- Every file exceeding 150 lines (with exact line count)
- Files/functions missing docstrings
- Specific naming violations

**Format:**
```
CODE QUALITY: X/30 points

[File size violations:]
âœ— 4 files exceed 150-line limit (-20 points)

  src/main.py: 215 lines (exceeds by 65)
    â†’ Split into: src/main.py, src/handlers.py, src/utils.py

  src/processor.py: 180 lines (exceeds by 30)
    â†’ Extract helper functions to separate module

  tests/test_all.py: 165 lines (exceeds by 15)
    â†’ Split into: test_unit.py, test_integration.py

[Docstring issues:]
âœ— Docstring coverage: 67% (target: 90%) (-6 points)

  Missing docstrings in:
    â€¢ src/utils.py:42 - function process_data()
    â€¢ src/handlers.py:15 - class DataHandler
    â€¢ src/models.py:89 - function validate_input()

  Fix: Add docstrings following format:
    def function_name():
        \"\"\"
        Brief description.

        Args:
            param: description

        Returns:
            description
        \"\"\"

[Naming violations:]
âœ— 3 naming convention violations (-1.5 points)

  â€¢ src/utils.py:12 - variable 'MyVariable' should be 'my_variable'
  â€¢ src/main.py:34 - function 'ProcessData' should be 'process_data'
  â€¢ src/config.py:8 - constant 'api_key' should be 'API_KEY'
```

#### ğŸ“š DOCUMENTATION (25 points)

**Show specifically:**
- Which documents are missing vs incomplete
- Word counts for each document
- Which required sections are missing

**Format:**
```
DOCUMENTATION: X/25 points

[Missing documents:]
âœ— PLANNING.md not found (-5 points)
  Create with sections:
    - Architecture Overview
    - Design Decisions
    - Technology Choices
    - Implementation Plan

[Incomplete documents:]
âš  TASKS.md: 89 words (too short, -3 points)
  Current sections: 2
  Missing sections:
    - Task breakdown with IDs
    - Status tracking (ğŸ”´ ğŸŸ¡ ğŸŸ¢)
    - Time estimates
    - Dependencies

  Add approximately 200+ more words with:
    - All development tasks
    - Task status updates
    - Completion tracking

âš  README.md: 245 words (needs improvement, -2 points)
  Missing sections:
    - Installation instructions
    - Usage examples with code
    - Troubleshooting guide

  Add:
    - Step-by-step setup guide
    - 3+ code examples
    - Common issues & solutions

[Complete documents:]
âœ“ PRD.md: 487 words âœ“
âœ“ CLAUDE.md: 312 words âœ“
```

#### ğŸ§ª TESTING (15 points)

**Show specifically:**
- Test file count
- Total test count
- Coverage percentage
- Which modules lack tests

**Format:**
```
TESTING: X/15 points

âœ“ Test files found: 5
âœ“ Total tests: 42
âš  Coverage: 68% (target: 70%, -2 points)

[Modules below 70% coverage:]
  â€¢ src/processor.py: 45% coverage
    Missing tests for:
      - process_data() function
      - error handling in validate()
      - edge cases in transform()

  â€¢ src/utils.py: 52% coverage
    Missing tests for:
      - file operations
      - exception handling
      - boundary conditions

[To improve coverage:]
  1. Add tests for src/processor.py:
     test_process_data_valid_input()
     test_process_data_invalid_input()
     test_process_data_edge_cases()

  2. Run coverage report:
     pytest --cov=src --cov-report=html
     open htmlcov/index.html
```

#### ğŸ”€ GIT WORKFLOW (10 points)

**Show specifically:**
- Exact commit count
- Examples of good/bad commit messages
- Missing commit message conventions

**Format:**
```
GIT WORKFLOW: X/10 points

Commits: 6 (minimum: 15, recommended: 15-25) (-5 points)

[Commit history analysis:]
âœ“ Good commits:
  â€¢ feat(core): Add data processing module [T-123]
  â€¢ docs(readme): Add installation instructions

âœ— Problematic commits:
  â€¢ "update" (too vague)
  â€¢ "fix" (no context)
  â€¢ "wip" (work-in-progress, should be completed)

[To improve:]
1. Make 9-19 more atomic commits showing:
   - Feature development progression
   - Testing additions
   - Documentation updates
   - Bug fixes

2. Follow conventional format:
   <type>(<scope>): <description> [TASK-ID]

   Types: feat, fix, docs, test, refactor, chore

   Examples:
   feat(auth): Implement user login system [T-101]
   test(auth): Add login validation tests [T-102]
   docs(api): Document authentication endpoints [T-103]

3. Show development progression:
   - Initial structure â†’ Implementation â†’ Testing â†’ Documentation
```

#### ğŸ”¬ RESEARCH (10 points)

**Show specifically:**
- What research documentation is missing
- Which prompts should be documented
- What analysis is lacking

**Format:**
```
RESEARCH: X/10 points

âš  Limited research documentation (-4 points)

[Missing:]
âœ— No prompts/ directory found
  Create: prompts/
    â”œâ”€â”€ README.md (lessons learned)
    â”œâ”€â”€ architecture/
    â”‚   â””â”€â”€ 001-design-decisions.md
    â”œâ”€â”€ code-generation/
    â”‚   â””â”€â”€ 001-implementation-prompts.md
    â””â”€â”€ testing/
        â””â”€â”€ 001-test-generation.md

âœ— No parameter exploration documented
  Add to PLANNING.md:
    - Parameters tested (3+ required)
    - Results comparison
    - Performance metrics

âœ— No comparative analysis
  Add section comparing:
    - Approach A vs Approach B
    - Trade-offs analysis
    - Final decision rationale

[To score full points:]
  1. Create prompts/ directory with 5+ documented prompts
  2. Document parameter exploration in PLANNING.md
  3. Add comparative analysis of design choices
```

### Step 3: Generate Prioritized Action Plan

Sort fixes by point value (highest impact first):

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PRIORITIZED ACTION PLAN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

To reach 70 (passing): Need +X points
To reach 85 (good):    Need +X points
To reach 90 (excellent): Need +X points

Priority 1: CODE QUALITY (+20 points potential)
  â–¡ Refactor src/main.py (215 â†’ <150 lines)
  â–¡ Refactor src/processor.py (180 â†’ <150 lines)
  â–¡ Refactor tests/test_all.py (165 â†’ <150 lines)
  â–¡ Add docstrings to 8 functions/classes
  â–¡ Fix 3 naming convention violations

Priority 2: DOCUMENTATION (+10 points potential)
  â–¡ Create PLANNING.md with architecture decisions
  â–¡ Expand TASKS.md to 300+ words with task tracking
  â–¡ Add installation section to README.md
  â–¡ Add usage examples to README.md

Priority 3: GIT WORKFLOW (+5 points potential)
  â–¡ Make 9-19 more atomic commits
  â–¡ Use conventional commit format
  â–¡ Show development progression

Priority 4: RESEARCH (+4 points potential)
  â–¡ Create prompts/ directory
  â–¡ Document 5+ prompt examples
  â–¡ Add parameter exploration analysis

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ESTIMATED TIME TO FIX: 4-6 hours
WITH FIXES: Current score + 39 points = XX/100 (Grade: X)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Step 4: Add Quick Reference Commands

Provide copy-paste commands for common fixes:

```
QUICK FIX COMMANDS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Check file sizes:
  python -c "from src.analyzers.file_size_analyzer import check_file_sizes; \
  violations = check_file_sizes('.', 150); \
  print('\\n'.join(f'{v.file_path}: {v.line_count} lines' for v in violations))"

Check docstring coverage:
  python -c "from src.analyzers.docstring_analyzer import analyze_project_docstrings; \
  result = analyze_project_docstrings('.'); \
  print(f\"Coverage: {result['coverage']:.1%}\"); \
  print('Missing:', result['missing'])"

Run test coverage:
  pytest --cov=src --cov-report=term-missing
  pytest --cov=src --cov-report=html
  open htmlcov/index.html

Check git commits:
  git log --oneline
  git log --format="%s" | head -20

Initialize missing files:
  touch PLANNING.md TASKS.md .env.example
  mkdir -p prompts/architecture prompts/code-generation prompts/testing
```

## Output Format

Generate a comprehensive markdown report and save it to the `results/` folder.

**Filename:** `results/<project_name>_detailed_report_YYYY-MM-DD_HHMMSS.md`

**Report structure:**

1. **Executive Summary**: Score, grade, pass/fail
2. **Category Details**: Each category with specific issues
3. **Action Plan**: Prioritized fixes by point value
4. **Quick Commands**: Copy-paste terminal commands
5. **Timeline**: Estimated effort to fix issues

### How to Generate the Report

Use the Python detailed reporter module:

```python
from datetime import datetime
from pathlib import Path
from src.reporters.detailed_reporter import generate_detailed_report
import json

# Load grading results
with open('grading_results.json', 'r') as f:
    results = json.load(f)

# Generate detailed report
project_name = Path(project_path).name
report_content = generate_detailed_report(results, project_path)

# Create results directory if it doesn't exist
Path('results').mkdir(exist_ok=True)

# Save to markdown file with timestamp
timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
report_filename = f"results/{project_name}_detailed_report_{timestamp}.md"

with open(report_filename, 'w', encoding='utf-8') as f:
    f.write(report_content)

print(f"\nâœ“ Detailed report saved to: {report_filename}")
```

Or use the command-line helper:

```bash
# After grading, generate detailed report
python -c "
from datetime import datetime
from pathlib import Path
from src.reporters.detailed_reporter import generate_detailed_report
import json
import sys

project_path = sys.argv[1]
results_file = sys.argv[2] if len(sys.argv) > 2 else 'grading_results.json'

# Load results
with open(results_file, 'r') as f:
    results = json.load(f)

# Generate report
report = generate_detailed_report(results, project_path)

# Save to file
Path('results').mkdir(exist_ok=True)
project_name = Path(project_path).name
timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
filename = f'results/{project_name}_detailed_report_{timestamp}.md'

with open(filename, 'w', encoding='utf-8') as f:
    f.write(report)

print(f'âœ“ Report saved: {filename}')
" <project_path> [results.json]
```

## Success Criteria

The detailed report should:
- âœ… Identify every specific file with issues
- âœ… Provide exact line numbers or locations
- âœ… Explain WHY each item lost points
- âœ… Show HOW to fix each issue
- âœ… Prioritize fixes by point value
- âœ… Include copy-paste commands
- âœ… Estimate time to complete fixes

## Example Usage

### Using the Skill Directly

```bash
/skill generate-detailed-report
```

Then provide:
- **Project path**: `./student-submission`
- **Results file**: `grading_results.json` (or paste results)

### Integrated with Grading

```bash
# Run all grading skills in parallel, then generate report

# Step 1: Run grading skills in parallel groups
# (See instructions above for parallel skill invocation)

# Step 2: After all skills complete, generate detailed report
/skill generate-detailed-report ./student-project

with open('grading_results.json') as f:
    results = json.load(f)

report = generate_detailed_report(results, './student-project')

Path('results').mkdir(exist_ok=True)
timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
filename = f'results/student-project_detailed_report_{timestamp}.md'

with open(filename, 'w') as f:
    f.write(report)

print(f'âœ“ Report saved: {filename}')
"
```

The skill generates:
- âœ… Markdown file in `results/` folder
- âœ… 200+ line detailed report
- âœ… Specific issues with file paths and line numbers
- âœ… Actionable recommendations
- âœ… Quick fix commands
- âœ… Timestamped filename for version control

## Integration with Grader

The orchestrator agent is configured to ALWAYS invoke this skill after grading:

```
User: "Grade ./my-project"

Agent: [Phase 1] Validates project
Agent: [Phase 2] Runs all 7 grading skills in parallel
Agent: [Phase 3] Aggregates results
       Score: 66/100 (D) - FAILED

Agent: [Phase 4] AUTOMATICALLY invokes /skill generate-detailed-report
       âœ“ Generating detailed report...
       âœ“ Report saved: results/my-project_detailed_report_2024-11-26_120530.md

Agent: [Presents summary and links to detailed report]

User can then:
- Review the markdown report for specific issues
- Follow actionable recommendations
- Use quick-fix commands
- Track progress as they fix issues
```

**Automatic Report Generation:**
The agent will ALWAYS create a detailed markdown report after grading, regardless of score. This ensures students have comprehensive feedback with specific file paths and actionable fixes.

## Notes

- Be specific: Always include file paths and line numbers
- Be actionable: Every issue should have a fix
- Be prioritized: Sort by point value (highest first)
- Be helpful: Include copy-paste commands
- Be encouraging: Frame as improvement opportunities
