# excel-feedback-generator Agent

**Version**: 1.0.0

**Purpose**: Generate instructor-facing Excel feedback sheets after RamiAutoGrader completes grading student submissions.

---

## Overview

This agent orchestrates three skills to transform grading results into a professionally formatted Excel file:

1. **extract-pdf-metadata**: Extract student info from PDF submissions
2. **generate-summary**: Create concise feedback summaries
3. **populate-excel**: Generate formatted .xlsx file

**Input**: Grading results directory + PDF submissions directory + assignment name

**Output**: `results/FinalFeedback_<assignment>.xlsx`

---

## Architecture

```
excel-feedback-generator (Agent)
├── extract-pdf-metadata (Skill) ──> Extract student ID, name, partner
├── generate-summary (Skill)     ──> Create 2-3 sentence feedback
└── populate-excel (Skill)       ──> Create formatted Excel file
```

**Data Flow**:
```
Student PDFs → extract-pdf-metadata → Metadata JSON
Grading Results → generate-summary → Summary Text
Both → populate-excel → Final Excel File
```

---

## Usage

### Standalone Execution

```bash
# Navigate to agent directory
cd ExcelFeedback/agents/excel-feedback-generator

# Run agent with Claude Code
claude-code run agent.yaml \
  --input grading_results_dir=../../results \
  --input pdf_submissions_dir=../../results \
  --input assignment_name="Design Patterns"
```

### Automatic Execution (Post-Grading Hook)

The agent is automatically triggered by RamiAutoGrader after grading completes.

**Integration Point**: `RamiAutoGrader/agents/grader-orchestrator/agent.yaml`

```yaml
post_grading_steps:
  - agent: excel-feedback-generator
    trigger: after_all_students_graded
    inputs:
      grading_results_dir: ${output_dir}
      pdf_submissions_dir: ${pdf_dir}
      assignment_name: ${assignment_name}
```

---

## Inputs

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `grading_results_dir` | string | Yes | Directory with grading results (e.g., `results/`) |
| `pdf_submissions_dir` | string | Yes | Directory with student PDFs (e.g., `results/`) |
| `assignment_name` | string | Yes | Assignment name for filename (e.g., "Design Patterns") |

**Expected Directory Structure**:
```
results/
├── student_name_1/
│   ├── grading_results.json    ← Grading scores and violations
│   ├── metadata.json            ← GitHub URL
│   └── student_submission.pdf   ← Original PDF
├── student_name_2/
│   ├── grading_results.json
│   ├── metadata.json
│   └── student_submission.pdf
└── ...
```

---

## Outputs

| Output | Type | Description |
|--------|------|-------------|
| `excel_file` | string | Path to generated Excel file |

**Output File**: `results/FinalFeedback_<assignment_name>.xlsx`

**Excel Structure**: 8 columns
1. Student ID (extracted from PDF)
2. Student Name (extracted from PDF)
3. Partner Name (extracted from PDF)
4. Assignment Name (extracted from PDF)
5. GitHub URL (from metadata.json)
6. Grade (from grading_results.json)
7. Summary (generated by Claude)
8. Notes (manual review flags)

---

## Workflow Steps

### Step 1: Validate Inputs
Verify that grading results and PDF directories exist.

**Error Handling**: Fails immediately if directories missing.

---

### Step 2: Extract Metadata from PDFs
For each student, extract metadata using Claude API.

**Input**: `student_submission.pdf`

**Output**: `pdf_metadata.json`
```json
{
  "student_id": "12345678",
  "student_name": "John Doe",
  "partner_name": "Jane Smith",
  "assignment_name": "Design Patterns",
  "confidence": 0.95,
  "extraction_status": "SUCCESS"
}
```

**Error Handling**: Continues with other students if one fails.

---

### Step 3: Generate Summaries
For each student, generate feedback summary from grading report.

**Input**: `grading_results.json`

**Output**: `summary.txt`
```
Score: 77/100. Strong documentation (25/25) and excellent security practices (10/10).
Code quality needs improvement: 2 files exceed 150-line limit. Increase test coverage from 67% to 70%.
```

**Error Handling**: Continues with other students if one fails.

---

### Step 4: Aggregate Student Data
Combine metadata, grading results, and summaries into single dataset.

**Output**: `aggregated_student_data.json`
```json
[
  {
    "student_id": "12345678",
    "student_name": "John Doe",
    "partner_name": "Jane Smith",
    "assignment_name": "Design Patterns",
    "github_url": "https://github.com/johndoe/project",
    "grade": "77/100",
    "summary": "Score: 77/100. Strong docs...",
    "notes": ""
  },
  ...
]
```

---

### Step 5: Create Excel Workbook
Generate formatted .xlsx file with all student data.

**Output**: `results/FinalFeedback_DesignPatterns.xlsx`

**Formatting**:
- Header row: Bold, light blue background (#D9E1F2), centered
- Borders: Thin borders on all cells
- Column widths: Student ID (12), Summary (60)
- Text wrapping: Enabled for Summary column
- GitHub URL: Clickable hyperlink

---

### Step 6: Verify Output
Verify Excel file was created successfully.

**Error Handling**: Fails if Excel file not found.

---

## Error Handling

### Skill-Level Errors

**Scenario**: PDF metadata extraction fails for one student

**Behavior**:
- Logs error to `results/excel_generation_errors.log`
- Continues processing other students
- Marks student with "EXTRACTION_ERROR" in Notes column

**Example**:
```
Student ID: NOT_FOUND
Name: NOT_FOUND
Notes: EXTRACTION_ERROR
```

---

### Critical Errors

**Scenario**: Excel generation fails (populate-excel skill error)

**Behavior**:
- Fails entire workflow
- Logs error details
- Returns error status to RamiAutoGrader agent

**Error Message**:
```
Error: Failed to generate Excel file. Check results/excel_generation_errors.log for details.
```

---

### Low Confidence Extractions

**Scenario**: PDF metadata extracted with confidence < 0.7

**Behavior**:
- Accepts extraction but flags for manual review
- Sets Notes column to "LOW_CONFIDENCE_REVIEW"
- Instructor manually verifies data

**Example**:
```
Student ID: ABC123 (invalid format)
Name: John
Partner: NOT_FOUND
Notes: LOW_CONFIDENCE_REVIEW
```

---

## Manual Review Flags

The Notes column contains flags for records requiring instructor attention:

| Flag | Meaning | Action Required |
|------|---------|-----------------|
| `LOW_CONFIDENCE_REVIEW` | Metadata extraction confidence < 70% | Verify student ID, name, partner |
| `PDF_MISSING` | Student PDF not found | Check if student submitted PDF |
| `EXTRACTION_ERROR` | Claude API failed after retries | Manual data entry needed |
| *(empty)* | All data extracted successfully | No action needed |

---

## Performance

**Typical Execution Time** (30 students):

| Step | Time |
|------|------|
| Validate inputs | < 1 second |
| Extract metadata (sequential) | 2-4 minutes |
| Generate summaries (sequential) | 1-3 minutes |
| Aggregate data | < 1 second |
| Create Excel | < 1 second |
| **Total** | **3-8 minutes** |

**Optimization**: Steps 2-3 can be parallelized (5 concurrent API calls) → **1-2 minutes total**

---

## Troubleshooting

### Issue: Excel file not created

**Symptoms**: Workflow completes but no Excel file found

**Causes**:
1. Permission error (cannot write to `results/` directory)
2. Invalid student data (empty list)
3. openpyxl library not installed

**Solutions**:
```bash
# Check permissions
ls -ld results/

# Check logs
cat results/excel_generation_errors.log

# Install dependencies
pip install -r requirements.txt
```

---

### Issue: "LOW_CONFIDENCE_REVIEW" for many students

**Symptoms**: > 50% of students flagged for manual review

**Causes**:
1. PDFs have inconsistent formatting
2. Student IDs not 8 digits
3. Partner names missing (solo projects)

**Solutions**:
1. Update PDF extraction prompt to handle variations
2. Adjust confidence scoring thresholds in `extract_metadata.py`
3. For solo projects, accept "NOT_FOUND" for partner_name

---

### Issue: Claude API rate limits

**Symptoms**: "Error: Failed to generate summary after 3 attempts"

**Causes**:
1. Tier 1 API limit: 50 requests/minute
2. Too many students processed simultaneously

**Solutions**:
```yaml
# In agent.yaml, add rate limiting:
rate_limiting:
  requests_per_minute: 45  # Leave buffer
  concurrent_requests: 5   # Max parallel
```

---

## Dependencies

**Python Packages** (from `requirements.txt`):
- `anthropic>=0.40.0` - Claude API client
- `pdfplumber>=0.10.0` - PDF text extraction
- `openpyxl>=3.1.0` - Excel generation
- `python-dotenv>=1.0.0` - Environment variables

**Environment Variables** (from `.env`):
- `ANTHROPIC_API_KEY` - Claude API key (required)

---

## Configuration

**Default Configuration** (`config/agent_config.yaml`):
```yaml
agent:
  timeout: 600  # 10 minutes
  retry_policy:
    max_retries: 3
    backoff_strategy: exponential

skills:
  extract_pdf_metadata:
    timeout: 30
    confidence_threshold: 0.7

  generate_summary:
    timeout: 20
    word_count_min: 30
    word_count_max: 50

  populate_excel:
    timeout: 60
    output_format: xlsx
```

**Customization**: Edit `config/agent_config.yaml` to adjust timeouts, thresholds, etc.

---

## Logs

**Agent Execution Log**: `results/excel_feedback_agent.log`

**Example**:
```
2025-11-28 14:32:01 - excel-feedback-generator - INFO - Starting workflow
2025-11-28 14:32:01 - excel-feedback-generator - INFO - Validating inputs
2025-11-28 14:32:02 - excel-feedback-generator - INFO - Extracting metadata for 30 students
2025-11-28 14:34:15 - excel-feedback-generator - INFO - Generating summaries for 30 students
2025-11-28 14:36:42 - excel-feedback-generator - INFO - Aggregating student data
2025-11-28 14:36:43 - excel-feedback-generator - INFO - Creating Excel workbook
2025-11-28 14:36:44 - excel-feedback-generator - INFO - ✓ Excel file created: results/FinalFeedback_DesignPatterns.xlsx
2025-11-28 14:36:44 - excel-feedback-generator - INFO - Workflow completed successfully
```

**Error Log**: `results/excel_generation_errors.log`

---

## Testing

**Unit Tests**: See `tests/unit/`

**Integration Test**:
```bash
# Run agent with sample data
pytest tests/integration/test_agent_workflow.py
```

**Manual Test**:
1. Run RamiAutoGrader on sample project
2. Verify `results/FinalFeedback_*.xlsx` created
3. Open in Microsoft Excel
4. Verify formatting, hyperlinks, data accuracy

---

## Success Criteria

- ✅ Excel file created with correct filename
- ✅ All students present in Excel (no missing rows)
- ✅ Metadata extracted correctly (≥80% confidence)
- ✅ Summaries generated for all students (30-50 words each)
- ✅ Manual review flags set appropriately
- ✅ File opens correctly in Microsoft Excel
- ✅ Hyperlinks are clickable
- ✅ Execution time < 10 minutes for 30 students

---

## Future Enhancements

- Parallel processing (extract + summarize concurrently)
- Conditional formatting (red for low grades, green for high)
- Charts showing grade distribution
- Email integration (auto-send feedback to students)
- Historical tracking (compare across assignments)

---

## Support

**Documentation**: See `docs/` directory
- `PRD.md` - Product requirements
- `PLANNING.md` - Architecture and ADRs
- `TASKS.md` - Task breakdown

**Issues**: Report bugs or feature requests to project maintainer

---

**Last Updated**: 2025-11-28
